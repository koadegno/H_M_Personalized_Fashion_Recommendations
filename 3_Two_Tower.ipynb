{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "### Two Tower Architecture\n",
    "This model is based on the two-tower architecture\n",
    "The two towers are used to learn representations of both the user and the item. The two-tower model is based on queries and the candidate they both shared a low-dimensional vector space. In our case, a query is customer and its transactions features and the candidate is the articles.\n",
    "\n",
    "I will try to use mlflow to keep track of experiments (and improve myself with this tool).\n",
    "\n",
    "\n",
    "\n",
    "![Alt text](https://miro.medium.com/v2/resize:fit:420/1*JbK2gjfLC4IFoM6AVWLaUQ.png)\n",
    "\n",
    "![Alt text](https://miro.medium.com/v2/resize:fit:420/format:webp/0*aJHT3_bGIvERfhaY)\n",
    "\n",
    "[image source](https://medium.com/smartnews-inc/user-behavior-sequence-for-items-recommendation-in-smartnews-ads-2376622f6192)\n",
    "\n",
    "\n",
    "\n",
    "Source: \n",
    "- https://medium.com/smartnews-inc/user-behavior-sequence-for-items-recommendation-in-smartnews-ads-2376622f6192\n",
    "- https://cloud.google.com/blog/products/ai-machine-learning/scaling-deep-retrieval-tensorflow-two-towers-architecture?hl=en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import preprocessing\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data and feature selections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before:  (105542, 25)\n",
      "Shape after:  (105542, 24)\n"
     ]
    }
   ],
   "source": [
    "articles_df = pd.read_csv(\"data/articles.csv\", encoding=\"utf-8\")\n",
    "print(\"Shape before: \", articles_df.shape)\n",
    "articles_df = preprocessing.preprocess_articles(articles_df)\n",
    "print(\"Shape after: \", articles_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before:  (1371980, 7)\n",
      "Shape after:  (1356119, 5)\n"
     ]
    }
   ],
   "source": [
    "customers_df = pd.read_csv(\"data/customers.csv\", encoding=\"utf-8\")\n",
    "print(\"Shape before: \", customers_df.shape)\n",
    "customers_df = preprocessing.preprocess_customers(customers_df)\n",
    "print(\"Shape after: \", customers_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before:  (31788324, 5)\n",
      "Shape after:  (31788324, 11)\n"
     ]
    }
   ],
   "source": [
    "transaction_df = pd.read_csv(\"data/transactions_train.csv\", encoding=\"utf-8\")\n",
    "print(\"Shape before: \", transaction_df.shape)\n",
    "transaction_df = preprocessing.preprocess_transactions(transaction_df)\n",
    "print(\"Shape after: \", transaction_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the three data sources to make the data compatible with out retrieval model.\n",
    "df = pd.merge(\n",
    "    pd.merge(\n",
    "        transaction_df[[\"article_id\", \"customer_id\", \"t_dat\", \"price\", \"month_sin\", \"month_cos\"]], articles_df[[\"article_id\", \"garment_group_name\", \"index_group_name\"]], on=\"article_id\", how=\"left\"\n",
    "    ),\n",
    "    customers_df[[\"customer_id\", \"age\", \"club_member_status\", \"age_group\"]],\n",
    "    on=\"customer_id\",\n",
    "    how=\"left\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>t_dat</th>\n",
       "      <th>price</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>month_cos</th>\n",
       "      <th>garment_group_name</th>\n",
       "      <th>index_group_name</th>\n",
       "      <th>age</th>\n",
       "      <th>club_member_status</th>\n",
       "      <th>age_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>663713001</td>\n",
       "      <td>000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...</td>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>0.050831</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>Under-, Nightwear</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>24.0</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>19-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>541518023</td>\n",
       "      <td>000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...</td>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>0.030492</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>Under-, Nightwear</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>24.0</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>19-25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  article_id                                        customer_id      t_dat  \\\n",
       "0  663713001  000058a12d5b43e67d225668fa1f8d618c13dc232df0ca... 2018-09-20   \n",
       "1  541518023  000058a12d5b43e67d225668fa1f8d618c13dc232df0ca... 2018-09-20   \n",
       "\n",
       "      price  month_sin  month_cos garment_group_name index_group_name   age  \\\n",
       "0  0.050831  -0.866025       -0.5  Under-, Nightwear       Ladieswear  24.0   \n",
       "1  0.030492  -0.866025       -0.5  Under-, Nightwear       Ladieswear  24.0   \n",
       "\n",
       "  club_member_status age_group  \n",
       "0             ACTIVE     19-25  \n",
       "1             ACTIVE     19-25  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31788324, 11)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Validate/Test split and Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2269ec532f0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "seed = 2024\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(df, test_size=0.1, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shape:  (28609491, 11)\n",
      "Validation shape:  (3178833, 11)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training shape: \", train_df.shape)\n",
    "print(\"Validation shape: \", val_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# class CachedDataset(Dataset):\n",
    "#     def __init__(self, df):\n",
    "#         self.encoders = {}\n",
    "#         self.data = {}\n",
    "\n",
    "#         # Process each column based on its type\n",
    "#         for col in df.columns:\n",
    "#             # Get column data\n",
    "#             col_data = df[col].values\n",
    "\n",
    "#             # Handle different data types\n",
    "#             if pd.api.types.is_numeric_dtype(df[col]) or col in [\"article_id\"]:\n",
    "#                 # For numeric data, convert to float32\n",
    "#                 self.data[col] = torch.tensor(col_data.astype(np.float32))\n",
    "\n",
    "#             else:\n",
    "#                 print(\"Col: \", col)\n",
    "#                 # For categorical/string data, use label encoding\n",
    "#                 self.encoders[col] = LabelEncoder()\n",
    "#                 encoded_data = self.encoders[col].fit_transform(col_data)\n",
    "#                 self.data[col] = torch.tensor(encoded_data, dtype=torch.long)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(next(iter(self.data.values())))\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return {key: value[idx] for key, value in self.data.items()}\n",
    "\n",
    "#     def get_feature_dims(self):\n",
    "#         \"\"\"Return the number of unique values for each categorical feature\"\"\"\n",
    "#         feature_dims = {}\n",
    "#         for col, encoder in self.encoders.items():\n",
    "#             feature_dims[col] = len(encoder.classes_)\n",
    "#         return feature_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class CachedDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.data = {col: torch.tensor(df[col].values) if pd.api.types.is_numeric_dtype(df[col]) else df[col].values for col in df.columns}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(next(iter(self.data.values())))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: value[idx] for key, value in self.data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64  # 2048\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CachedDataset(train_df)\n",
    "val_dataset = CachedDataset(val_df)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'article_id': array(['898983001', '826760003'], dtype=object),\n",
       " 'customer_id': array(['1a7980d01d2858df0d407e65140b0a50ef875c970cd65781a2b6251fca5e8861',\n",
       "        '1f014e0075e19fa35dbd9f731f72b2b649571890b7ab3bdb164cb385d5cb1b72'],\n",
       "       dtype=object),\n",
       " 't_dat': array(['2020-05-18T00:00:00.000000000', '2020-01-17T00:00:00.000000000'],\n",
       "       dtype='datetime64[ns]'),\n",
       " 'price': tensor([0.0254, 0.0137], dtype=torch.float64),\n",
       " 'month_sin': tensor([0.8660, 0.0000], dtype=torch.float64),\n",
       " 'month_cos': tensor([-0.5000,  1.0000], dtype=torch.float64),\n",
       " 'garment_group_name': array(['Shoes', 'Unknown'], dtype=object),\n",
       " 'index_group_name': array(['Ladieswear', 'Ladieswear'], dtype=object),\n",
       " 'age': tensor([32., 26.], dtype=torch.float64),\n",
       " 'club_member_status': array(['ACTIVE', 'ACTIVE'], dtype=object),\n",
       " 'age_group': ['26-35', '26-35']\n",
       " Categories (8, object): ['0-18' < '19-25' < '26-35' < '36-45' < '46-55' < '56-65' < '66-80' < '80+']}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_list = train_df[\"customer_id\"].unique().tolist()\n",
    "item_id_list = train_df[\"article_id\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "garment_group_list = train_df[\"garment_group_name\"].unique().tolist()\n",
    "index_group_list = train_df[\"index_group_name\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transactions: 28,609,491\n",
      "Number of users: 1,347,924\n",
      "Number of items: 104,060\n",
      "['Shoes', 'Unknown', 'Jersey Basic', 'Trousers', 'Jersey Fancy', 'Dressed', 'Under-, Nightwear', 'Knitwear', 'Skirts', 'Shirts', 'Trousers Denim', 'Outdoor', 'Swimwear', 'Dresses Ladies', 'Blouses', 'Accessories', 'Shorts', 'Socks and Tights', 'Special Offers', 'Dresses/Skirts girls', 'Woven/Jersey/Knitted mix Baby']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of transactions: {len(train_df):,}\")\n",
    "print(f\"Number of users: {len(user_id_list):,}\")\n",
    "print(f\"Number of items: {len(item_id_list):,}\")\n",
    "print(garment_group_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Tower Model\n",
    "\n",
    "- Query model: Generates a query representation given user and transaction features.\n",
    "- Candidate model: Generates an item representation given item features.\n",
    "\n",
    "**Both models produce embeddings that live in the same embedding space.**\n",
    "\n",
    "\n",
    "For the query embedding I will use:\n",
    "- `customer_id`: ID of the customer.\n",
    "- `age`: age of the customer at the time of purchase.\n",
    "- `month_sin`, `month_cos`: time of year the purchase was made.\n",
    "\n",
    "For the candidate embedding I will use:\n",
    "- `article_id`: ID of the item.\n",
    "- `garment_group_name`: type of garment.\n",
    "- `index_group_name`: menswear/ladieswear etc.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StringLookup:\n",
    "\n",
    "    def __init__(self, vocabulary: List[str], mask_token=None):\n",
    "        self.vocab = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "        self.vocab[\"<UNK>\"] = len(self.vocab)\n",
    "        self.mask_token = mask_token\n",
    "\n",
    "    def __call__(self, inputs: np.ndarray) -> torch.Tensor:\n",
    "        # Convert string inputs to indices\n",
    "        indices = [self.vocab.get(x, self.vocab[\"<UNK>\"]) for x in inputs]\n",
    "        return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocab)\n",
    "\n",
    "\n",
    "class StringEmbedding(nn.Module):\n",
    "    def __init__(self, user_id_list: List[str], embedding_dimension: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create the vocabulary space\n",
    "        self.string_lookup = StringLookup(user_id_list)\n",
    "\n",
    "        # The real embeddings\n",
    "        self.embedding = nn.Embedding(num_embeddings=len(self.string_lookup), embedding_dim=embedding_dimension)\n",
    "\n",
    "    def forward(self, user_ids: torch.Tensor) -> torch.Tensor:\n",
    "        # Convert user IDs to indices\n",
    "        return self.embedding(self.string_lookup(user_ids))\n",
    "\n",
    "    # def forward(self, user_ids):\n",
    "    #     user_indices = torch.tensor([self.user_to_index.get(user_id, self.user_to_index[\"<UNK>\"]) for user_id in user_ids], device=self.embedding.weight.device)\n",
    "    #     return self.embedding(user_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "\n",
    "class QueryTower(nn.Module):\n",
    "\n",
    "    def __init__(self, user_id_list: List[str], embedding_dimension: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.user_embedding = StringEmbedding(user_id_list, embedding_dimension)\n",
    "        self.age_normalization = nn.BatchNorm1d(1)\n",
    "\n",
    "        self.dense_nn = nn.Sequential(nn.Linear(in_features=embedding_dimension + 3, out_features=embedding_dimension), nn.ReLU(), nn.Linear(embedding_dimension, embedding_dimension))\n",
    "\n",
    "    def forward(self, inputs: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "        user_embedding = self.user_embedding(inputs[\"customer_id\"])\n",
    "        normalized_age = self.age_normalization(inputs[\"age\"].float().unsqueeze(1))\n",
    "        month_sin = inputs[\"month_sin\"].float().unsqueeze(1)\n",
    "        month_cos = inputs[\"month_cos\"].float().unsqueeze(1)\n",
    "\n",
    "        concatenated_inputs = torch.cat([user_embedding, normalized_age, month_sin, month_cos], dim=1)\n",
    "\n",
    "        outputs = self.dense_nn(concatenated_inputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QueryTower(user_id_list, 128)(train_dataset[0:2]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The candidate model is very similar to the query model.\n",
    "\n",
    "it has two categorical features which will be one-hot encode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemTower(nn.Module):\n",
    "\n",
    "    def __init__(self, item_id_list: List[str], garment_group_list: List[str], index_group_list: List[str], embedding_dimension: int):\n",
    "        super().__init__()\n",
    "        self.item_embedding = StringEmbedding(item_id_list, embedding_dimension)\n",
    "\n",
    "        # Garment group setup\n",
    "        self.garment_group_lookup = StringLookup(vocabulary=garment_group_list)\n",
    "        self.garment_group_size = len(garment_group_list)\n",
    "\n",
    "        # Index group setup\n",
    "        self.index_group_lookup = StringLookup(vocabulary=index_group_list)\n",
    "        self.index_group_size = len(index_group_list)\n",
    "\n",
    "        input_dim = embedding_dimension + self.garment_group_size + self.index_group_size\n",
    "        self.dense_nn = nn.Sequential(nn.Linear(input_dim, embedding_dimension), nn.ReLU(), nn.Linear(embedding_dimension, embedding_dimension))\n",
    "\n",
    "    def forward(self, inputs: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "\n",
    "        # Convert article_id strings to embeddings\n",
    "        item_embedding = self.item_embedding(inputs[\"article_id\"])\n",
    "\n",
    "        garment_indices = self.garment_group_lookup(inputs[\"garment_group_name\"])\n",
    "        garment_one_hot = torch.zeros((garment_indices.size(0), self.garment_group_size), dtype=torch.float32)\n",
    "        garment_one_hot.scatter_(1, garment_indices.unsqueeze(1), 1.0)\n",
    "\n",
    "        # Convert index group strings to one-hot encodings\n",
    "        index_indices = self.index_group_lookup(inputs[\"index_group_name\"])\n",
    "        index_one_hot = torch.zeros((index_indices.size(0), self.index_group_size), dtype=torch.float32)\n",
    "        index_one_hot.scatter_(1, index_indices.unsqueeze(1), 1.0)\n",
    "\n",
    "        # Concatenate all features\n",
    "        concatenated = torch.cat([item_embedding, garment_one_hot, index_one_hot], dim=1)\n",
    "\n",
    "        outputs = self.dense_nn(concatenated)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ItemTower(item_id_list, garment_group_list, index_group_list, 128)(train_dataset[0:2]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from typing import Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class TwoTowerModel(nn.Module):\n",
    "    def __init__(self, query_tower: nn.Module, item_tower: nn.Module):\n",
    "        super().__init__()\n",
    "        self.query_tower = query_tower\n",
    "        self.item_tower = item_tower\n",
    "\n",
    "    def forward(self, batch: Dict) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        query_embeddings = self.query_tower(batch[\"query\"])\n",
    "        item_embeddings = self.item_tower(batch[\"candidate\"])\n",
    "\n",
    "        # Normalize embeddings\n",
    "        query_embeddings = F.normalize(query_embeddings, p=2, dim=1)\n",
    "        item_embeddings = F.normalize(item_embeddings, p=2, dim=1)\n",
    "\n",
    "        return query_embeddings, item_embeddings\n",
    "\n",
    "    def compute_loss(self, query_embeddings: torch.Tensor, item_embeddings: torch.Tensor) -> torch.Tensor:\n",
    "        # Compute similarity matrix\n",
    "        similarities = torch.matmul(query_embeddings, item_embeddings.t())\n",
    "\n",
    "        # Create labels for the batch (diagonal is positive pairs)\n",
    "        labels = torch.arange(query_embeddings.size(0), device=query_embeddings.device)\n",
    "\n",
    "        # Compute cross entropy loss\n",
    "        loss = F.cross_entropy(similarities, labels)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_item_embeddings_index(model: TwoTowerModel, unique_items_loader: DataLoader, device: torch.device) -> Tuple[torch.Tensor, np.ndarray]:\n",
    "    \"\"\"Create an index of all unique item embeddings.\"\"\"\n",
    "    model.eval()\n",
    "    all_embeddings = []\n",
    "    all_item_ids = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(unique_items_loader, desc=\"Creating item embeddings index\"):\n",
    "            item_embeddings = model.item_tower(batch)\n",
    "            item_embeddings = F.normalize(item_embeddings, p=2, dim=1)\n",
    "            all_embeddings.append(item_embeddings.cpu())\n",
    "            all_item_ids.extend(batch[\"article_id\"])\n",
    "\n",
    "    return torch.cat(all_embeddings, dim=0), np.array(all_item_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_top_k(model: TwoTowerModel, val_loader: DataLoader, item_embeddings: torch.Tensor, item_ids: np.ndarray, k: int = 100, device: torch.device = None) -> float:\n",
    "    \"\"\"Evaluate the model using top-k accuracy.\"\"\"\n",
    "    model.eval()\n",
    "    total_hits = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "            # Get query embeddings\n",
    "            query_embeddings = model.query_tower(batch[\"query\"])\n",
    "            query_embeddings = F.normalize(query_embeddings, p=2, dim=1)\n",
    "\n",
    "            # Compute similarities with all items\n",
    "            similarities = torch.matmul(query_embeddings, item_embeddings.t())\n",
    "\n",
    "            # Get top-k items\n",
    "            _, top_k_indices = similarities.topk(k)\n",
    "\n",
    "            # Get actual item IDs that were purchased\n",
    "            actual_items = batch[\"candidate\"][\"article_id\"]\n",
    "\n",
    "            # Check if actual items are in top-k predictions\n",
    "            for actual_item, top_k_idx in zip(actual_items, top_k_indices):\n",
    "                predicted_items = item_ids[top_k_idx.cpu()]\n",
    "                if actual_item in predicted_items:\n",
    "                    total_hits += 1\n",
    "                total_samples += 1\n",
    "\n",
    "    return total_hits / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model: TwoTowerModel, train_loader: DataLoader, optimizer: torch.optim.Optimizer, device: torch.device) -> float:\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Move batch to device\n",
    "        batch = {\n",
    "            \"query\": {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch[\"query\"].items()},\n",
    "            \"candidate\": {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch[\"candidate\"].items()},\n",
    "        }\n",
    "\n",
    "        # Forward pass\n",
    "        query_embeddings, item_embeddings = model(batch)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = model.compute_loss(query_embeddings, item_embeddings)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "def train_and_evaluate(model: TwoTowerModel, train_loader: DataLoader, val_loader: DataLoader, unique_items_loader: DataLoader, num_epochs: int = 10, device: torch.device = None):\n",
    "    \"\"\"Train and evaluate the model.\"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "\n",
    "        # Create item embeddings index\n",
    "        item_embeddings, item_ids = create_item_embeddings_index(model, unique_items_loader, device)\n",
    "        item_embeddings = item_embeddings.to(device)\n",
    "\n",
    "        # Evaluate\n",
    "        top_k_accuracy = evaluate_top_k(model, val_loader, item_embeddings, item_ids, k=100, device=device)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Top-100 Accuracy: {top_k_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/447024 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Create dataset of unique items\n",
    "unique_items_df = train_df.drop_duplicates(\"article_id\")[[\"article_id\", \"garment_group_name\", \"index_group_name\"]]\n",
    "unique_items_dataset = CachedDataset(unique_items_df)\n",
    "unique_items_loader = DataLoader(unique_items_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=11, pin_memory=True)\n",
    "\n",
    "# Initialize model\n",
    "model = TwoTowerModel(query_tower=QueryTower(user_id_list, embedding_dimension=32), item_tower=ItemTower(item_id_list, garment_group_list, index_group_list, embedding_dimension=32))\n",
    "\n",
    "# Train and evaluate\n",
    "train_and_evaluate(model=model, train_loader=train_loader, val_loader=val_loader, unique_items_loader=unique_items_loader, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
